{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Extract features</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Image features</h3>\n",
    "<p>Image feature is a simple image pattern, based on which we can describe what we see on the image. The main role of features in computer vision(and not only) is to transform visual information into the vector space. This give us possibility to perform mathematical operations on them, for example finding similar vector(which lead us to similar image or object on the image)</p>\n",
    "\n",
    "<div class=\"imgHolder\">\n",
    "    <img src = \"Kazekeypoints.png\"></img>\n",
    "    <span><h2 style=\"text-align:center;color:red;\"> Kaze keypoints </h2></span>\n",
    "    <img src = \"HOG.png\"></img>\n",
    "    <span><h2 style=\"text-align:center;color:red;\"> Original and image with HOG features </h2></span>\n",
    "</div>\n",
    "\n",
    "<h3>How do we get features from images</h3>\n",
    "<p>There are two ways of getting features from image\n",
    "    <ul>\n",
    "        <li>first is an image descriptors(white box algorithms)</li>\n",
    "        <li>second is a neural nets(black box algorithms)</li>\n",
    "        </ul>\n",
    "We will work with the first one using the OpenCV library.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import required libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import os # reading data\n",
    "import cv2 # reading images\n",
    "import pickle as cpickle # store data for fast processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setup the proper locations for the datasets folders</h3>\n",
    "<p>Dataset can be found <a href=\"https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\">here</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataDir = \"C:\\\\Users\\\\thodo\\\\Downloads\\\\archive\\\\chest_xray\\\\train\"\n",
    "testDataDir = \"C:\\\\Users\\\\thodo\\\\Downloads\\\\archive\\\\chest_xray\\\\test\"\n",
    "validateDataDir = \"C:\\\\Users\\\\thodo\\\\Downloads\\\\archive\\\\chest_xray\\\\val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initialize dictionaries in which we will store the data of each category</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {}\n",
    "testing_data = {}\n",
    "validate_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"NORMAL\", \"PNEUMONIA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Kaze descriptor</h3>\n",
    "<p>KAZE detector is based on scale normalized determinant of Hessian Matrix which is computed at multiple scale levels. The maxima of detector response are picked up as feature-points using a moving window. Feature description introduces the property of rotation invariance by finding dominant orientation in a circular  neighborhood  around  each  detected  feature.  KAZE features are invariant to rotation, scale, limited affine and have more distinctiveness at varying scales with the cost  of moderate increase in computational time.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract data using KAZE descriptor function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Kaze_features(image):\n",
    "    try:\n",
    "        alg = cv2.KAZE_create()\n",
    "        # Dinding image keypoints\n",
    "        kps = alg.detect(image)\n",
    "        # Getting first 32 of them.\n",
    "        # Number of keypoints is varies depend on image size and color pallet\n",
    "        # Sorting them based on keypoint response value(bigger is better)\n",
    "        vector_size = 32\n",
    "        kps = sorted(kps, key=lambda x: -x.response)[:vector_size]\n",
    "        # computing descriptors vector\n",
    "        kps, dsc = alg.compute(image, kps)\n",
    "        # Flatten all of them in one big vector - our feature vector\n",
    "        dsc = dsc.flatten()\n",
    "        # Making descriptor of same size\n",
    "        # Descriptor vector size is 64\n",
    "        needed_size = (vector_size * 64)\n",
    "        if dsc.size < needed_size:\n",
    "            # if we have less the 32 descriptors then just adding zeros at the\n",
    "            # end of our feature vector\n",
    "            dsc = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])\n",
    "        return dsc\n",
    "    except cv2.error as e:\n",
    "        print('Error: ' + e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>HOG descriptor</h3>\n",
    "\n",
    "<p>The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The technique counts occurrences of gradient orientation in localized portions of an image. This method is similar to that of edge orientation histograms, scale-invariant feature transform descriptors, and shape contexts, but differs in that it is computed on a dense grid of uniformly spaced cells and uses overlapping local contrast normalization for improved accuracy. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract data using HOG descriptor function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Hog_Features(image):\n",
    "    try:\n",
    "        cell_size = (10, 10)  # h x w in pixels\n",
    "        block_size = (2, 2)  # h x w in cells\n",
    "        nbins = 9  # number of orientation bins\n",
    "\n",
    "        # winSize is the size of the image cropped to an multiple of the cell size\n",
    "        hog = cv2.HOGDescriptor(_winSize=(image.shape[1] // cell_size[1] * cell_size[1],\n",
    "                                          image.shape[0] // cell_size[0] * cell_size[0]),\n",
    "                                _blockSize=(block_size[1] * cell_size[1],\n",
    "                                            block_size[0] * cell_size[0]),\n",
    "                                _blockStride=(cell_size[1], cell_size[0]),\n",
    "                                _cellSize=(cell_size[1], cell_size[0]),\n",
    "                                _nbins=nbins)\n",
    "\n",
    "        n_cells = (image.shape[0] // cell_size[0], image.shape[1] // cell_size[1])\n",
    "        dsc = hog.compute(image) \\\n",
    "            .reshape(n_cells[1] - block_size[1] + 1,\n",
    "                     n_cells[0] - block_size[0] + 1,\n",
    "                     block_size[0], block_size[1], nbins) \\\n",
    "            .transpose((1, 0, 2, 3, 4))\n",
    "        return dsc.flatten()\n",
    "    except cv2.error as e:\n",
    "        print('Error: ' + e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generic function for feature extraction</h3>\n",
    "<p>Function needs as parameters:\n",
    "<ol>\n",
    "  <li>Image path</li>\n",
    "  <li>Descriptor (Empty, KAZE, HOG)</li>\n",
    "</ol>\n",
    "Returns an array.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path, extractFeaturesUsing = ''):\n",
    "    # make sure that image is grayscale\n",
    "    image_array = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # resize image to 100'100\n",
    "    image = cv2.resize(image_array, (100, 100))\n",
    "    if extractFeaturesUsing == '':\n",
    "        return image.flatten()\n",
    "    elif extractFeaturesUsing == 'KAZE':\n",
    "        return Get_Kaze_features(image)\n",
    "    elif extractFeaturesUsing == 'HOG':\n",
    "        return Get_Hog_Features(image)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load data function</h3>\n",
    "<p>Function needs as parameters:\n",
    "<ol>\n",
    "  <li>Dictionary that we are trying to construct (one of the three we initialized at the start).</li>\n",
    "  <li>Directory location.</li>\n",
    "  <li>Name of pickle in which we will store the data for fast access.</li>\n",
    "  <li>Descriptor with which we will extract the features (None, KAZE, HOG).</li>\n",
    "</ol>\n",
    "After the load function finishes, stores the dictionary into a pickle file and so we are now able to load the data again really fast.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(dictionaryToStoreData, dataDir, pickleName, extractFeaturesUsing = ''):\n",
    "    # load training data\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataDir, category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            name = img.split('/')[-1].lower()\n",
    "            try:\n",
    "                imageLocation = os.path.join(path, img)\n",
    "                features = extract_features(imageLocation, extractFeaturesUsing)\n",
    "                dictionaryToStoreData[imageLocation] = [features, class_num]\n",
    "            except:\n",
    "                print(\"An exception occurred while extracting features from image \" + name)\n",
    "    # saving all our feature vectors in pickled file\n",
    "    with open(pickleName + '.pickle', 'wb') as fp:\n",
    "        cpickle.dump(dictionaryToStoreData, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Example of LoadData use </h3>\n",
    "<p> Load data using HOG descriptor</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadData(training_data, trainDataDir, 'trainingDataUsingHog', 'HOG')\n",
    "LoadData(testing_data, testDataDir, 'testingDataUsingHog', 'HOG')\n",
    "LoadData(validate_data, validateDataDir, 'validateDataUsingHog', 'HOG')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
